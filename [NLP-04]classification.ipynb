{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binding-drawing",
   "metadata": {},
   "source": [
    "# classification project\n",
    "GD 노드 4\n",
    "\n",
    "프로세스는 다음과 같다.\n",
    "1. 데이터 로드\n",
    "2. 데이터 전처리\n",
    "3. 머신러닝 모델 사용\n",
    "4. 딥러닝 모델 사용\n",
    "5. 비교 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-cigarette",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드\n",
    "단어 수를 각각 10000, 5000, 2500 으로 하여 데이터를 로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "random-wheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file PosixPath('/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/mpl-data/matplotlibrc'), line 250 ('font.family:  NanumGothic')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "covered-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n",
    "(x_train_2, y_train_2), (x_test_2, y_test_2) = reuters.load_data(num_words=5000, test_split=0.2)\n",
    "(x_train_3, y_train_3), (x_test_3, y_test_3) = reuters.load_data(num_words=2500, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-petite",
   "metadata": {},
   "source": [
    "### 데이터 확인\n",
    "훈련 데이터는 8982, 테스트 데이터는 2246개이고,   \n",
    "클래수의 수는 46개이며, 데이터의 길이는 평균 145 정도이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liquid-animal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 수: 8982\n",
      "테스트 샘플의 수: 2246\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 수: {}'.format(len(x_train)))\n",
    "print('테스트 샘플의 수: {}'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "allied-pulse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "[1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "executive-street",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "printable-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스의 수 : 46\n"
     ]
    }
   ],
   "source": [
    "num_classes = max(y_train) + 1\n",
    "print('클래스의 수 : {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spectacular-picking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
    "print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "environmental-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(12,5)\n",
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "determined-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 클래스 빈도수:\n",
      "[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "    14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "    28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
      "    42   43   44   45]\n",
      " [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\n",
      "    26   20  444   39   66  549  269  100   15   41   62   92   24   15\n",
      "    48   19   45   39   32   11   50   10   49   19   19   24   36   30\n",
      "    13   21   12   18]]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(\"각 클래스 빈도수:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-arthur",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "자연어를 처리하기 위해서 BoW 방식을 사용할 것이고    \n",
    "이를 위해서 DTM을 만들고 tfidf를 적용할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-shoot",
   "metadata": {},
   "source": [
    "### 사전 구축 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grateful-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crucial-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index + 3 : word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "early-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "filled-choir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_to_word[index] for index in x_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wanted-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abroad-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_2)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_2[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_2 = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_2[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_2 = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "expected-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_2)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_3[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_3 = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_3[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_3 = decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-dutch",
   "metadata": {},
   "source": [
    "### DTM & TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unlimited-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n"
     ]
    }
   ],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "print(x_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "naval-istanbul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "print(tfidfv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-mirror",
   "metadata": {},
   "source": [
    "## 3. 머신러닝 모델 사용\n",
    "TFIDF을 적용한 데이터를 이용하여 8가지 머신러닝 모델에 적용을 한다.   \n",
    "모델은 다음과 같다.   \n",
    "나이브 베이즈 분류기, CNB, 로지스틱 회귀, 서포트 벡터 머신, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리, 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "suitable-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(train, test):\n",
    "    dtmvector = CountVectorizer()\n",
    "    train_dtm = dtmvector.fit_transform(train)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidfv = tfidf_transformer.fit_transform(train_dtm)\n",
    "    \n",
    "    test_dtm = dtmvector.transform(test) #테스트 데이터를 DTM으로 변환\n",
    "    tfidfv_test = tfidf_transformer.transform(test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
    "    return tfidfv, tfidfv_test\n",
    "\n",
    "# 나이브 베이즈 분류기, CNB, 로지스틱 회귀, 서포트 벡터 머신, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리, 보팅\n",
    "def pipeline(x_train, y_train, x_test, y_test):\n",
    "    # tfidf\n",
    "    (tfidfv, tfidfv_test) = tfidf(x_train,x_test)\n",
    "    \n",
    "    # multi_NB\n",
    "    print('multi_NB')\n",
    "    mod = MultinomialNB()\n",
    "    mod.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, mod.predict(tfidfv_test)))\n",
    "    \n",
    "    # CN\n",
    "    print('CNB')\n",
    "    cb = ComplementNB()\n",
    "    cb.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, cb.predict(tfidfv_test)))\n",
    "\n",
    "    # logistic regression\n",
    "    print('logistic')\n",
    "    lr = LogisticRegression(C=10000, penalty='l2')\n",
    "    lr.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, lr.predict(tfidfv_test)))\n",
    "    \n",
    "    # SVM\n",
    "    print('SVM')\n",
    "    lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "    lsvc.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, lsvc.predict(tfidfv_test)))\n",
    "\n",
    "    # DT\n",
    "    print('DT')\n",
    "    tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "    tree.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, tree.predict(tfidfv_test)))\n",
    "\n",
    "    # RF\n",
    "    print('RF')\n",
    "    forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "    forest.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, forest.predict(tfidfv_test)))\n",
    "    \n",
    "    # GRBT\n",
    "    print('GRBT')\n",
    "    grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "    grbt.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, grbt.predict(tfidfv_test)))\n",
    "\n",
    "    # voting\n",
    "    print('voting')\n",
    "    voting = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "    ], voting='soft', n_jobs=-1)\n",
    "    voting.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, voting.predict(tfidfv_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "subject-institution",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_NB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.62      0.69      0.65       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.81      0.90      0.85       813\n",
      "           4       0.51      0.96      0.67       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.08      0.15        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.66      0.63      0.64        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.03      0.05        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.69      0.56      0.61        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.60      0.78      0.68       133\n",
      "          20       1.00      0.04      0.08        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.03      0.06        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66      2246\n",
      "   macro avg       0.17      0.10      0.10      2246\n",
      "weighted avg       0.59      0.66      0.58      2246\n",
      "\n",
      "CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.64      0.88      0.74       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.75      0.92      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.50      0.13      0.21        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.55      0.73      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.58      0.59      0.59        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.79      0.73        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.75      0.30      0.43        70\n",
      "          21       0.74      0.63      0.68        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.50      0.60        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.85      0.74      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.25      0.10      0.14        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.75      0.77      0.75      2246\n",
      "\n",
      "logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.78      0.76       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.92      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.59      0.59      0.59        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.68      0.75      0.71        99\n",
      "          17       0.75      0.75      0.75        12\n",
      "          18       0.86      0.60      0.71        20\n",
      "          19       0.68      0.68      0.68       133\n",
      "          20       0.62      0.49      0.54        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.64      0.58      0.61        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.57      0.40      0.47        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.88      0.58      0.70        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.36      0.36      0.36        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.71      0.62      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n",
      "SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.68      0.72      0.70       105\n",
      "           2       0.78      0.70      0.74        20\n",
      "           3       0.90      0.92      0.91       813\n",
      "           4       0.80      0.84      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.71      0.64        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.89      0.83      0.86        30\n",
      "          11       0.62      0.70      0.66        83\n",
      "          12       0.40      0.31      0.35        13\n",
      "          13       0.60      0.57      0.58        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.43      0.33      0.38         9\n",
      "          16       0.67      0.73      0.70        99\n",
      "          17       0.80      0.33      0.47        12\n",
      "          18       0.81      0.65      0.72        20\n",
      "          19       0.66      0.67      0.67       133\n",
      "          20       0.51      0.46      0.48        70\n",
      "          21       0.59      0.81      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.43      0.25      0.32        12\n",
      "          24       0.47      0.47      0.47        19\n",
      "          25       0.85      0.74      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.40      0.44        10\n",
      "          29       0.33      0.50      0.40         4\n",
      "          30       0.62      0.42      0.50        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.50      0.43      0.46         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.56      0.45      0.50        11\n",
      "          37       0.50      1.00      0.67         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.20      0.29         5\n",
      "          40       0.50      0.30      0.37        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.67      0.58      0.60      2246\n",
      "weighted avg       0.78      0.78      0.78      2246\n",
      "\n",
      "DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.42      0.53       105\n",
      "           2       0.62      0.50      0.56        20\n",
      "           3       0.93      0.83      0.88       813\n",
      "           4       0.40      0.90      0.56       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.90      0.64      0.75        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.64      0.51      0.56        83\n",
      "          12       0.14      0.08      0.10        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.84      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.29      0.39       133\n",
      "          20       0.27      0.06      0.09        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      1.00      1.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.23      0.18      0.18      2246\n",
      "weighted avg       0.61      0.62      0.58      2246\n",
      "\n",
      "RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.33      0.27        12\n",
      "           1       0.45      0.77      0.57       105\n",
      "           2       0.30      0.30      0.30        20\n",
      "           3       0.82      0.90      0.86       813\n",
      "           4       0.61      0.83      0.70       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.67      0.53      0.59        38\n",
      "           9       0.70      0.28      0.40        25\n",
      "          10       0.75      0.30      0.43        30\n",
      "          11       0.55      0.59      0.57        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.37      0.19      0.25        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.59      0.59        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.50      0.25      0.33        20\n",
      "          19       0.69      0.54      0.61       133\n",
      "          20       0.57      0.29      0.38        70\n",
      "          21       0.67      0.30      0.41        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       1.00      0.32      0.49        31\n",
      "          26       1.00      0.25      0.40         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.50      0.08      0.14        12\n",
      "          31       0.67      0.15      0.25        13\n",
      "          32       0.67      0.20      0.31        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.50      0.14      0.22         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.67      0.33      0.44         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.46      0.27      0.31      2246\n",
      "weighted avg       0.66      0.67      0.64      2246\n",
      "\n",
      "GRBT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.77      0.68      0.72       105\n",
      "           2       0.78      0.70      0.74        20\n",
      "           3       0.88      0.91      0.89       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.80      0.86      0.83        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.74      0.80      0.77        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.63      0.64      0.63        83\n",
      "          12       0.33      0.46      0.39        13\n",
      "          13       0.62      0.49      0.55        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.38      0.33      0.35         9\n",
      "          16       0.73      0.73      0.73        99\n",
      "          17       0.27      0.25      0.26        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.69      0.65      0.67       133\n",
      "          20       0.67      0.46      0.54        70\n",
      "          21       0.70      0.78      0.74        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.54      0.58      0.56        12\n",
      "          24       0.61      0.58      0.59        19\n",
      "          25       0.89      0.55      0.68        31\n",
      "          26       0.71      0.62      0.67         8\n",
      "          27       0.50      0.50      0.50         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.23      0.75      0.35         4\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.62      0.38      0.48        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.75      0.60      0.67         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       0.80      0.67      0.73         6\n",
      "          36       0.62      0.45      0.53        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.40      0.20      0.27        10\n",
      "          41       0.56      0.62      0.59         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.67      0.67      0.67         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.58      0.58      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n",
      "voting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.77      0.74      0.76       105\n",
      "           2       0.73      0.80      0.76        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.83      0.88      0.85       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.70      0.68      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.65      0.69      0.67        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.57      0.44      0.50         9\n",
      "          16       0.72      0.75      0.73        99\n",
      "          17       0.53      0.67      0.59        12\n",
      "          18       0.79      0.55      0.65        20\n",
      "          19       0.68      0.69      0.68       133\n",
      "          20       0.62      0.49      0.54        70\n",
      "          21       0.65      0.81      0.72        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.60      0.75      0.67        12\n",
      "          24       0.67      0.63      0.65        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.40      1.00      0.57         4\n",
      "          30       0.67      0.50      0.57        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.56      0.45      0.50        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline(x_train, y_train, x_test, y_test) # voca 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "finnish-verse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_NB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.50      0.80      0.62       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.86      0.89      0.87       813\n",
      "           4       0.59      0.95      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.48      0.73      0.58        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.14      0.24        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.66      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.51      0.81      0.63       133\n",
      "          20       0.90      0.13      0.23        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.16      0.12      0.11      2246\n",
      "weighted avg       0.60      0.67      0.60      2246\n",
      "\n",
      "CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.63      0.86      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.74      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.57      0.21      0.31        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.54      0.76      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.69      0.59      0.64        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.67      0.79      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.56      0.80      0.66       133\n",
      "          20       0.79      0.33      0.46        70\n",
      "          21       0.78      0.67      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.33      0.44        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.77      0.81        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.76      0.77      0.75      2246\n",
      "\n",
      "logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.80      0.79       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.74      0.68        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.64      0.62      0.63        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.67      0.73      0.70        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.66      0.68      0.67       133\n",
      "          20       0.61      0.47      0.53        70\n",
      "          21       0.62      0.78      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.69      0.58      0.63        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.67      0.40      0.50        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       0.70      0.54      0.61        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.38      0.27      0.32        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.73      0.61      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n",
      "SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.72      0.72      0.72       105\n",
      "           2       0.60      0.75      0.67        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.81      0.84      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.61      0.66      0.63        38\n",
      "           9       0.77      0.80      0.78        25\n",
      "          10       0.77      0.80      0.79        30\n",
      "          11       0.64      0.73      0.69        83\n",
      "          12       0.40      0.31      0.35        13\n",
      "          13       0.59      0.54      0.56        37\n",
      "          14       1.00      0.50      0.67         2\n",
      "          15       0.50      0.44      0.47         9\n",
      "          16       0.66      0.70      0.68        99\n",
      "          17       0.60      0.25      0.35        12\n",
      "          18       0.87      0.65      0.74        20\n",
      "          19       0.61      0.65      0.63       133\n",
      "          20       0.45      0.43      0.44        70\n",
      "          21       0.55      0.78      0.65        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.54      0.58      0.56        12\n",
      "          24       0.57      0.42      0.48        19\n",
      "          25       0.85      0.55      0.67        31\n",
      "          26       1.00      0.75      0.86         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.22      0.50      0.31         4\n",
      "          30       0.62      0.42      0.50        12\n",
      "          31       0.67      0.31      0.42        13\n",
      "          32       0.89      0.80      0.84        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.50      0.57      0.53         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.56      0.45      0.50        11\n",
      "          37       0.50      1.00      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.50      0.60      0.55         5\n",
      "          40       0.38      0.30      0.33        10\n",
      "          41       0.36      0.50      0.42         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.66      0.58      0.59      2246\n",
      "weighted avg       0.77      0.77      0.77      2246\n",
      "\n",
      "DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.40      0.52       105\n",
      "           2       0.60      0.45      0.51        20\n",
      "           3       0.94      0.84      0.89       813\n",
      "           4       0.39      0.91      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.57      0.73        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.48      0.54        83\n",
      "          12       0.17      0.08      0.11        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.82      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.26      0.37       133\n",
      "          20       0.33      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.24      0.17      0.18      2246\n",
      "weighted avg       0.61      0.62      0.57      2246\n",
      "\n",
      "RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.42      0.33        12\n",
      "           1       0.42      0.78      0.55       105\n",
      "           2       0.44      0.35      0.39        20\n",
      "           3       0.84      0.90      0.87       813\n",
      "           4       0.68      0.84      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.43      0.57        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.53      0.56        38\n",
      "           9       0.71      0.40      0.51        25\n",
      "          10       0.89      0.53      0.67        30\n",
      "          11       0.57      0.69      0.62        83\n",
      "          12       0.33      0.15      0.21        13\n",
      "          13       0.46      0.32      0.38        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       1.00      0.11      0.20         9\n",
      "          16       0.70      0.67      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.60      0.45      0.51        20\n",
      "          19       0.62      0.64      0.63       133\n",
      "          20       0.46      0.33      0.38        70\n",
      "          21       0.65      0.41      0.50        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.25      0.38        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.87      0.42      0.57        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.25      0.12      0.17         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.54      0.31      0.36      2246\n",
      "weighted avg       0.69      0.70      0.68      2246\n",
      "\n",
      "GRBT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.80      0.68      0.73       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.14      0.20      0.17         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.91      0.84      0.87        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.66      0.64        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.55      0.43      0.48        37\n",
      "          14       0.08      0.50      0.14         2\n",
      "          15       0.33      0.22      0.27         9\n",
      "          16       0.72      0.77      0.75        99\n",
      "          17       0.33      0.33      0.33        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.71      0.65      0.68       133\n",
      "          20       0.56      0.44      0.50        70\n",
      "          21       0.67      0.67      0.67        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.36      0.42      0.38        12\n",
      "          24       0.71      0.63      0.67        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.40      0.50      0.44         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.22      0.50      0.31         4\n",
      "          30       0.38      0.42      0.40        12\n",
      "          31       0.60      0.46      0.52        13\n",
      "          32       0.88      0.70      0.78        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.67      0.55      0.60        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.25      0.33      0.29         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.71      0.50      0.59        10\n",
      "          41       0.44      0.50      0.47         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.50      0.67      0.57         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.60      0.59      0.58      2246\n",
      "weighted avg       0.77      0.77      0.77      2246\n",
      "\n",
      "voting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.71      0.85      0.77        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.72      0.68      0.70        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.67      0.70      0.68        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.12      0.50      0.20         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.74      0.74      0.74        99\n",
      "          17       0.57      0.67      0.62        12\n",
      "          18       0.72      0.65      0.68        20\n",
      "          19       0.73      0.68      0.71       133\n",
      "          20       0.61      0.49      0.54        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.75      0.63      0.69        19\n",
      "          25       0.96      0.74      0.84        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.62      0.42      0.50        12\n",
      "          31       0.75      0.69      0.72        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.80      0.40      0.53        10\n",
      "          41       0.67      0.50      0.57         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline(x_train_2, y_train_2, x_test_2, y_test_2) # voca 5,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hired-template",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_NB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40        12\n",
      "           1       0.48      0.81      0.60       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.89      0.87      0.88       813\n",
      "           4       0.63      0.94      0.76       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.75      0.08      0.14        38\n",
      "           9       1.00      0.52      0.68        25\n",
      "          10       1.00      0.13      0.24        30\n",
      "          11       0.44      0.77      0.56        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.75      0.16      0.27        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.54      0.77      0.63        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.80      0.20      0.32        20\n",
      "          19       0.50      0.81      0.62       133\n",
      "          20       0.94      0.23      0.37        70\n",
      "          21       1.00      0.30      0.46        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.10      0.18        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       1.00      0.14      0.25         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.28      0.15      0.16      2246\n",
      "weighted avg       0.67      0.69      0.64      2246\n",
      "\n",
      "CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57        12\n",
      "           1       0.62      0.84      0.71       105\n",
      "           2       0.83      0.50      0.62        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.73      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.79      0.85        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.67      0.21      0.32        38\n",
      "           9       0.85      0.92      0.88        25\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       0.53      0.78      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.64      0.57      0.60        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.63      0.76      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.50      0.60      0.55        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.86      0.34      0.49        70\n",
      "          21       0.71      0.56      0.63        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.79      0.71      0.75        31\n",
      "          26       0.86      0.75      0.80         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.25      0.20      0.22        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.08      0.14        13\n",
      "          32       1.00      0.50      0.67        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       1.00      0.86      0.92         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.20      0.33         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.57      0.38      0.42      2246\n",
      "weighted avg       0.75      0.76      0.73      2246\n",
      "\n",
      "logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.74      0.74      0.74       105\n",
      "           2       0.67      0.80      0.73        20\n",
      "           3       0.89      0.92      0.91       813\n",
      "           4       0.78      0.84      0.81       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.63      0.68      0.66        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.89      0.83      0.86        30\n",
      "          11       0.61      0.69      0.65        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.60      0.68      0.63        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.67      0.70      0.68        99\n",
      "          17       0.80      0.67      0.73        12\n",
      "          18       0.75      0.60      0.67        20\n",
      "          19       0.66      0.67      0.66       133\n",
      "          20       0.52      0.46      0.49        70\n",
      "          21       0.67      0.74      0.70        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.58      0.58      0.58        12\n",
      "          24       0.64      0.47      0.55        19\n",
      "          25       0.92      0.71      0.80        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.43      0.30      0.35        10\n",
      "          29       0.60      0.75      0.67         4\n",
      "          30       0.80      0.33      0.47        12\n",
      "          31       0.67      0.46      0.55        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.38      0.27      0.32        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.55      1.00      0.71         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.75      0.58      0.63      2246\n",
      "weighted avg       0.78      0.78      0.78      2246\n",
      "\n",
      "SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.58      0.67        12\n",
      "           1       0.65      0.65      0.65       105\n",
      "           2       0.75      0.75      0.75        20\n",
      "           3       0.88      0.88      0.88       813\n",
      "           4       0.77      0.80      0.79       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.63      0.61        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.89      0.80      0.84        30\n",
      "          11       0.59      0.66      0.62        83\n",
      "          12       0.33      0.23      0.27        13\n",
      "          13       0.53      0.49      0.51        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.60      0.33      0.43         9\n",
      "          16       0.58      0.64      0.61        99\n",
      "          17       1.00      0.33      0.50        12\n",
      "          18       0.77      0.50      0.61        20\n",
      "          19       0.60      0.62      0.61       133\n",
      "          20       0.49      0.49      0.49        70\n",
      "          21       0.59      0.63      0.61        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.50      0.58      0.54        12\n",
      "          24       0.60      0.47      0.53        19\n",
      "          25       0.71      0.65      0.68        31\n",
      "          26       0.86      0.75      0.80         8\n",
      "          27       0.50      0.25      0.33         4\n",
      "          28       0.40      0.20      0.27        10\n",
      "          29       0.25      0.50      0.33         4\n",
      "          30       0.50      0.33      0.40        12\n",
      "          31       0.56      0.38      0.45        13\n",
      "          32       0.78      0.70      0.74        10\n",
      "          33       0.57      0.80      0.67         5\n",
      "          34       0.40      0.57      0.47         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.31      0.36      0.33        11\n",
      "          37       0.20      0.50      0.29         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.33      0.20      0.25         5\n",
      "          40       0.43      0.30      0.35        10\n",
      "          41       0.27      0.38      0.32         8\n",
      "          42       0.50      0.67      0.57         3\n",
      "          43       0.60      1.00      0.75         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.74      2246\n",
      "   macro avg       0.59      0.54      0.54      2246\n",
      "weighted avg       0.74      0.74      0.74      2246\n",
      "\n",
      "DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.74      0.40      0.52       105\n",
      "           2       0.62      0.40      0.48        20\n",
      "           3       0.94      0.85      0.89       813\n",
      "           4       0.41      0.91      0.56       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.64      0.78        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.87      0.80      0.83        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.56      0.58      0.57        83\n",
      "          12       0.14      0.08      0.10        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.82      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.65      0.27      0.38       133\n",
      "          20       0.22      0.03      0.05        70\n",
      "          21       1.00      0.04      0.07        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.50      0.06      0.11        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       1.00      0.10      0.18        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.63      2246\n",
      "   macro avg       0.26      0.18      0.18      2246\n",
      "weighted avg       0.62      0.63      0.58      2246\n",
      "\n",
      "RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40        12\n",
      "           1       0.47      0.83      0.60       105\n",
      "           2       0.53      0.40      0.46        20\n",
      "           3       0.84      0.89      0.87       813\n",
      "           4       0.67      0.84      0.74       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.62      0.66      0.64        38\n",
      "           9       0.67      0.56      0.61        25\n",
      "          10       0.86      0.63      0.73        30\n",
      "          11       0.59      0.63      0.61        83\n",
      "          12       0.29      0.15      0.20        13\n",
      "          13       0.48      0.41      0.44        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.59      0.59        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.50      0.20      0.29        20\n",
      "          19       0.66      0.57      0.61       133\n",
      "          20       0.67      0.41      0.51        70\n",
      "          21       0.71      0.44      0.55        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.50      0.33      0.40        12\n",
      "          24       0.60      0.16      0.25        19\n",
      "          25       0.87      0.42      0.57        31\n",
      "          26       0.50      0.12      0.20         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.25      0.25      0.25         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.50      0.67        10\n",
      "          33       0.75      0.60      0.67         5\n",
      "          34       0.50      0.14      0.22         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.18      0.27        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.67      0.67      0.67         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.71      2246\n",
      "   macro avg       0.48      0.34      0.38      2246\n",
      "weighted avg       0.68      0.71      0.68      2246\n",
      "\n",
      "GRBT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61        12\n",
      "           1       0.77      0.68      0.72       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.93      0.91      0.92       813\n",
      "           4       0.73      0.84      0.78       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.20      0.33      0.25         3\n",
      "           8       0.59      0.61      0.60        38\n",
      "           9       0.69      0.72      0.71        25\n",
      "          10       0.83      0.83      0.83        30\n",
      "          11       0.59      0.64      0.61        83\n",
      "          12       0.38      0.38      0.38        13\n",
      "          13       0.60      0.49      0.54        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.33      0.40         9\n",
      "          16       0.74      0.74      0.74        99\n",
      "          17       0.36      0.33      0.35        12\n",
      "          18       0.60      0.60      0.60        20\n",
      "          19       0.72      0.63      0.67       133\n",
      "          20       0.59      0.49      0.53        70\n",
      "          21       0.57      0.59      0.58        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.61      0.58      0.59        19\n",
      "          25       0.90      0.61      0.73        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.33      0.25      0.29         4\n",
      "          28       0.43      0.30      0.35        10\n",
      "          29       0.29      0.50      0.36         4\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.43      0.46      0.44        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.33      0.29      0.31         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.60      0.55      0.57        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.17      0.20      0.18         5\n",
      "          40       0.38      0.30      0.33        10\n",
      "          41       0.17      0.12      0.14         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.56      0.55      0.54      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n",
      "voting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.77      0.74      0.76       105\n",
      "           2       0.71      0.85      0.77        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.64      0.71      0.67        38\n",
      "           9       0.74      0.80      0.77        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.62      0.67      0.64        83\n",
      "          12       0.56      0.38      0.45        13\n",
      "          13       0.62      0.68      0.65        37\n",
      "          14       0.33      0.50      0.40         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.72      0.73      0.72        99\n",
      "          17       0.57      0.67      0.62        12\n",
      "          18       0.70      0.70      0.70        20\n",
      "          19       0.71      0.69      0.70       133\n",
      "          20       0.57      0.43      0.49        70\n",
      "          21       0.72      0.78      0.75        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.60      0.75      0.67        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.92      0.71      0.80        31\n",
      "          26       0.78      0.88      0.82         8\n",
      "          27       0.50      0.25      0.33         4\n",
      "          28       0.43      0.30      0.35        10\n",
      "          29       0.40      0.50      0.44         4\n",
      "          30       0.71      0.42      0.53        12\n",
      "          31       0.77      0.77      0.77        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.62      0.45      0.53        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.20      0.20      0.20         5\n",
      "          40       0.40      0.20      0.27        10\n",
      "          41       0.60      0.38      0.46         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.66      0.61      0.62      2246\n",
      "weighted avg       0.80      0.80      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pipeline(x_train_3, y_train_3, x_test_3, y_test_3) # voca 2,500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-sponsorship",
   "metadata": {},
   "source": [
    "## 4. 딥러닝 모델 사용\n",
    "평균적으로 가장 좋았던 어휘 크기가 5,000에 대하여 딥러닝 모델을 적용한다.   \n",
    "정확한 비교를 위해서 tfidf를 적용한 데이터를 그대로 사용한다.   \n",
    "모델은 간단하게 denes layer 2개 층(입출력 층 제외)을 쌓은 모델이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "reduced-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(100, 100), random_state=100)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "(tfidfv, tfidfv_test) = tfidf(x_train_2,x_test_2)\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(100, 100), random_state=100)\n",
    "clf.fit(tfidfv, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hidden-chile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.77      0.70      0.73       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.91      0.92      0.92       813\n",
      "           4       0.78      0.88      0.83       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.76      0.88      0.81        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.65      0.80      0.72        83\n",
      "          12       0.38      0.23      0.29        13\n",
      "          13       0.61      0.59      0.60        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.67      0.22      0.33         9\n",
      "          16       0.70      0.72      0.71        99\n",
      "          17       0.50      0.25      0.33        12\n",
      "          18       0.86      0.60      0.71        20\n",
      "          19       0.65      0.69      0.67       133\n",
      "          20       0.64      0.59      0.61        70\n",
      "          21       0.73      0.70      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.44      0.33      0.38        12\n",
      "          24       0.69      0.58      0.63        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.80      1.00      0.89         4\n",
      "          28       0.36      0.40      0.38        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       1.00      0.58      0.74        12\n",
      "          31       0.88      0.54      0.67        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.71      0.45      0.56        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.43      0.38      0.40         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.70      0.60      0.63      2246\n",
      "weighted avg       0.79      0.80      0.79      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj28/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_2, clf.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-sucking",
   "metadata": {},
   "source": [
    "## 5. 비교 분석\n",
    "어휘 크기에 따른 머신러닝의 성능은 크게 다르지 않았다. 즉, 정확도와 weighted f1 score 기준으로 1% 정도의 차이가 났다.   \n",
    "10000, 5000, 2500 중에서 5000이 가장 좋은 성능을 보여주고 2500이 가장 낮은 성능을 보여준다. 즉, 어휘 크기가 너무 많거나 적어도 안 좋다고 볼 수 있다.   \n",
    "가장 좋았던 모델은 앙상블을 사용한 voting 모델로 5,000 크기에서 정확도 82%, f1 score(weighted avg) 81%이다.   \n",
    "대부분의 모델들은 정확도와 f1 score에서 70% 이상의 성능을 보여줬고, naive bayesion, decision tree와 random forest는 70%에 못 미치는 성능을 보여준다. 아마 앙상블 적용하여 편향적인 과적합을 어느 정도 해결할 수 있어서 앙상블 모델이 가장 좋은 성능을 보여준 것 같다.   \n",
    "딥러닝의 모델은 어휘의 크기가 5000일 때로 하였고, 성는은 정확도 80%, f1 79%이다. 가장 좋은 모델인 앙상블에 조금 못 미치지만 비슷하다.   \n",
    "사용한 모델이 dense layer이기 때문에 결국 머신러닝의 stacking 방법을 사용한 앙상블이기 떄문에 비슷한 성능을 보여주는 것 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-proof",
   "metadata": {},
   "source": [
    "## 회고 및 루브릭 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-aging",
   "metadata": {},
   "source": [
    "### 루브릭 평가 항목\n",
    "1. 분류 모델의 accuracy가 기준 이상 높게 나왔는가?\n",
    "\t(3가지 단어 개수에 대해 8가지 머신러닝 기법을 적용하여 그중 최적의 솔루션을 도출하였다.)\n",
    "2. 분류 모델의 F1 score가 기준 이상 높게 나왔는가?\n",
    "\t(Vocabulary size에 따른 각 머신러닝 모델의 성능변화 추이를 살피고, 해당 머신러닝 알고리즘의 특성에 근거해 원인을 분석하였다.)\n",
    "3. 생성모델의 metric(BLEU 등) 기준 이상 높은 성능이 확인되었는가?\n",
    "\t(동일한 데이터셋과 전처리 조건으로 딥러닝 모델의 성능과 비교하여 결과에 따른 원인을 분석하였다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-outline",
   "metadata": {},
   "source": [
    "### 평가 항목에 대한 수행\n",
    "1. 3가지 단어 개수에 대해 8가지 머신러닝 기법을 적용하였고 그 중 앙상블 방법 중 voting이 가장 좋았다.\n",
    "2. vocabulary size에 따른 모델의 성능을 비교한 결과 중간 정도가 가장 좋고 너무 적거나 많아도 안 좋은 성능을 보여줌을 나타냈고, 앙상블 모델의 특성을 설명하여 가장 좋은 모델임을 보였다.\n",
    "3. 동일한 데이터셋과 전처리 조건으로 딥러닝 모델의 성능과 비교했고, 그에 따른 원인을 분석했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-reunion",
   "metadata": {},
   "source": [
    "### 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-seeker",
   "metadata": {},
   "source": [
    "노션에 공지된 꼭 포함이 되어야 할 점\n",
    "- 이번 프로젝트에서 **어려웠던 점,**\n",
    "- 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**.\n",
    "- 루브릭 평가 지표를 맞추기 위해 **시도한 것들**.\n",
    "- 만약에 루브릭 평가 관련 지표를 **달성 하지 못했을 때, 이유에 관한 추정**.\n",
    "- **자기 다짐**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-password",
   "metadata": {},
   "source": [
    "---\n",
    "- **어려웠던 점**    \n",
    "루브릭 항목에서 기준 이상이란 표현을 사용하는데 어디에도 '기준'이 없어서 혼란스러웠고, 머신러닝 모델을 훈련하는데에 생각보다 오래 걸렸다. \n",
    "\n",
    "---\n",
    "- **알아낸 점**    \n",
    "sklearn에서 간단하게 mlp을 구현할 수 있도록 API를 제공하는 것을 알 수 있었다.\n",
    "\n",
    "- **모호한 점**    \n",
    "단순히 nlp에서 여러 머신러닝 모델을 적용하라고 하는데 이게 nlp 부분이랑 구체적으로 무슨 연관이 있는지는 모르겠다. 내 생각에는 오히려 여러 가지 머신러닝 기법을 배우는 노드인 것 같은 느낌이다. \n",
    "\n",
    "---\n",
    "- **시도한 것들**   \n",
    "어휘 크기를 10000, 5000, 2500으로 하였고, 8가지 머신러닝 기법과 딥러닝(신경망)을 적용하였다.\n",
    "\n",
    "---\n",
    "- **루브릭 평가 관련 지표**   \n",
    "제 예상에는 모두 달성되었다고 생각한다. 그 이유는 위에 있는 **평가 항목에 대한 수행**에 나와있다.  \n",
    "- **자기 다짐** 및 **나의 생각들**      \n",
    "NLP 노드인데 기대 이하의 노드였다. 그냥 단순히 여러 가지 머신러닝을 적용해 보는 것이기 때문에 nlp 관련해서 한 것은 벡터화 밖에 없다.(이것도 새로운 개념은 아니다.) 왜 이게 going deeper nlp 노드에 있는 지 모르겠다. 정말 시간이 아까운 노드이다.   \n",
    "그리고 노드에서는 RNN이나 1D CNN 등을 적용해보라고 한다. 그런데 전처리가 동일하면 BoW 기반의 DTM을 사용하고 이것은 이미 순서의 정보가 없어서 RNN이나 1D CNN을 사용하는 의미가 없어지는 데 왜 이런 것들을 적용하는 지 이해를 할 수가 없었다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-nurse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
